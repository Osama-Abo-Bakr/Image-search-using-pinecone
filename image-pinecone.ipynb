{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Image Search Using Pinecone and ConvBase For Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Main Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "# import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "# model\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Prepare Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-sample\\00357563a7.jpg</td>\n",
       "      <td>3054</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data-sample\\003bd60fa9.jpg</td>\n",
       "      <td>3055</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-sample\\01c6b7230c.jpg</td>\n",
       "      <td>3056</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data-sample\\024a037366.jpg</td>\n",
       "      <td>3057</td>\n",
       "      <td>class-b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data-sample\\029c926ce9.jpg</td>\n",
       "      <td>3058</td>\n",
       "      <td>class-a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         path    id    class\n",
       "0  data-sample\\00357563a7.jpg  3054  class-a\n",
       "1  data-sample\\003bd60fa9.jpg  3055  class-b\n",
       "2  data-sample\\01c6b7230c.jpg  3056  class-a\n",
       "3  data-sample\\024a037366.jpg  3057  class-b\n",
       "4  data-sample\\029c926ce9.jpg  3058  class-a"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathes = [os.path.join('data-sample', image) for image in os.listdir('data-sample')]\n",
    "df = pd.DataFrame({'path': pathes,\n",
    "                   'id': np.arange(3054, 3054 + len(pathes), 1),\n",
    "                   'class': ['class-a', 'class-b'] * int(len(pathes)/2)}\n",
    "                  )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Load Envirnoment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(override=True)\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Model: VGG19 for Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (1): ConvMlp(\n",
       "    (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (act1): ReLU(inplace=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (act2): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model('vgg19', pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "_ = model.eval()\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Extract Image Feature`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_feature(image_paths: list):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    batch_fearures = []\n",
    "    for image_path in image_paths:\n",
    "        # Convert Image Path to Pillow\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0)\n",
    "        \n",
    "        # Pass The Image to Model to Extract Feature.\n",
    "        with torch.no_grad():\n",
    "            conv_feature = model(image)\n",
    "            \n",
    "            image_features = conv_feature.view(conv_feature.size(0), -1).tolist()[0]\n",
    "        \n",
    "        # append Feature\n",
    "        batch_fearures.append(image_features)\n",
    "    \n",
    "    return batch_fearures\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectore Feature Length: 4096\n"
     ]
    }
   ],
   "source": [
    "results = extract_image_feature(image_paths=['data-sample/0a73823599.jpg', 'data-sample/866a4779a7.jpg'])\n",
    "\n",
    "vect_length = len(results[0])\n",
    "print(f'Vectore Feature Length: {vect_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Upserting to Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes\n",
      "Creating Index: image-search-live\n",
      "Done Creating Index: image-search-live\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x1d5e566b2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "try:\n",
    "    print('Deleting all indexes')\n",
    "    _ = [pinecone.delete_index(name=index_name['name']) for index_name in pinecone.list_indexes()]\n",
    "except Exception as e:\n",
    "    print('Error In Deleting Indexes: {}'.format(e))\n",
    "    \n",
    "    \n",
    "index_name = 'image-search-live'\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    print('Creating Index: {}'.format(index_name))\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        dimension=vect_length,\n",
    "        metric='cosine',\n",
    "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "    )\n",
    "    print('Done Creating Index: {}'.format(index_name))\n",
    "    \n",
    "    \n",
    "index = pinecone.Index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:57<00:00, 14.64s/it]\n"
     ]
    }
   ],
   "source": [
    "def upserting_to_pinecone(df_images, batch_size=32):\n",
    "    faild_ids = []\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(df_images), batch_size)):\n",
    "    \n",
    "        try:\n",
    "            batch_end = min(len(df_images), batch_start + batch_size)\n",
    "            \n",
    "            paths_batch = df_images['path'][batch_start:batch_end].tolist()\n",
    "            ids_batch = df_images['id'][batch_start:batch_end].astype(str).tolist()\n",
    "            metadata_classes = df_images['class'][batch_start:batch_end].tolist()\n",
    "            \n",
    "            # Call to Extract Image Feature.\n",
    "            batch_extracted = extract_image_feature(image_paths=paths_batch)\n",
    "            \n",
    "            # Prepare Data To Upserting\n",
    "            to_upsert = [(ids, features, {'class': cls}) for ids, features, cls in zip(ids_batch, batch_extracted, metadata_classes)]\n",
    "            \n",
    "            # To Upserting in Pinecone\n",
    "            _ = index.upsert(vectors=to_upsert)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Faild upserting {e}')\n",
    "            faild_ids.append(ids_batch)\n",
    "    \n",
    "    return faild_ids\n",
    "\n",
    "\n",
    "## Apply the Upserting Function\n",
    "faild_ids = upserting_to_pinecone(df_images=df, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Query In Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3293',\n",
       "  'metadata': {'class': 'class-b'},\n",
       "  'score': 0.999999881,\n",
       "  'values': []},\n",
       " {'id': '3160',\n",
       "  'metadata': {'class': 'class-a'},\n",
       "  'score': 0.776371419,\n",
       "  'values': []},\n",
       " {'id': '3224',\n",
       "  'metadata': {'class': 'class-a'},\n",
       "  'score': 0.760004163,\n",
       "  'values': []},\n",
       " {'id': '3164',\n",
       "  'metadata': {'class': 'class-a'},\n",
       "  'score': 0.729620099,\n",
       "  'values': []},\n",
       " {'id': '3135',\n",
       "  'metadata': {'class': 'class-b'},\n",
       "  'score': 0.701402307,\n",
       "  'values': []}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the similer pages\n",
    "\n",
    "image_path_new = df['path'].iloc[-1]\n",
    "\n",
    "image_feaures = extract_image_feature(image_paths=[image_path_new])[0]\n",
    "\n",
    "# Query In Pinecone\n",
    "result = index.query(vector=[image_feaures], top_k=5, include_metadata=True,)\n",
    "result['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Deleting In Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = index.delete(ids=['3327', '3152'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 4096,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 239}},\n",
       " 'total_vector_count': 239}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Inserting In Pinecone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Fetch\n",
    "\n",
    "index.fetch(ids=['3293'])['vectors']['3293']['values'][:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectorDB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
